{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9HWkj42Bp9SlXw587V9y/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monsterglitch/Text-Generation/blob/master/Training%26Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q7UDbfIGrC2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1f4c41-c558-4617-8555-54d3ea8a4186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 0123456789abcdefghijklmnopqrstuvwxyz﻿\n",
            "Number of characters: 158596\n",
            "Number of unique characters: 39\n",
            "Input: ﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
            "\n",
            "\n",
            "\n",
            "this ebook is for the use of\n",
            "Target:  \n",
            "Input shape: (100, 39)\n",
            "Target shape: (39,)\n",
            "================================================== \n",
            "\n",
            "Input: project gutenbergs alices adventures in wonderland by lewis carroll\n",
            "\n",
            "\n",
            "\n",
            "this ebook is for the use of \n",
            "Target: a\n",
            "Input shape: (100, 39)\n",
            "Target shape: (39,)\n",
            "================================================== \n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100, 256)          303104    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 39)                10023     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 838439 (3.20 MB)\n",
            "Trainable params: 838439 (3.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1238/1238 [==============================] - 47s 34ms/step - loss: 2.2291 - accuracy: 0.3563\n",
            "Epoch 2/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 1.7194 - accuracy: 0.4898\n",
            "Epoch 3/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 1.5202 - accuracy: 0.5417\n",
            "Epoch 4/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 1.3749 - accuracy: 0.5820\n",
            "Epoch 5/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 1.2520 - accuracy: 0.6166\n",
            "Epoch 6/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 1.1429 - accuracy: 0.6461\n",
            "Epoch 7/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 1.0406 - accuracy: 0.6747\n",
            "Epoch 8/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.9503 - accuracy: 0.7006\n",
            "Epoch 9/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.8702 - accuracy: 0.7224\n",
            "Epoch 10/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.7936 - accuracy: 0.7463\n",
            "Epoch 11/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.7266 - accuracy: 0.7646\n",
            "Epoch 12/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.6685 - accuracy: 0.7836\n",
            "Epoch 13/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.6141 - accuracy: 0.8002\n",
            "Epoch 14/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.5700 - accuracy: 0.8133\n",
            "Epoch 15/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.5291 - accuracy: 0.8257\n",
            "Epoch 16/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.4957 - accuracy: 0.8365\n",
            "Epoch 17/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.4610 - accuracy: 0.8484\n",
            "Epoch 18/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.4380 - accuracy: 0.8545\n",
            "Epoch 19/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.4110 - accuracy: 0.8628\n",
            "Epoch 20/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.3942 - accuracy: 0.8677\n",
            "Epoch 21/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.3727 - accuracy: 0.8756\n",
            "Epoch 22/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.3609 - accuracy: 0.8791\n",
            "Epoch 23/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.3439 - accuracy: 0.8848\n",
            "Epoch 24/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.3316 - accuracy: 0.8885\n",
            "Epoch 25/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.3165 - accuracy: 0.8932\n",
            "Epoch 26/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.3073 - accuracy: 0.8965\n",
            "Epoch 27/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.2988 - accuracy: 0.8992\n",
            "Epoch 28/30\n",
            "1238/1238 [==============================] - 39s 32ms/step - loss: 0.2888 - accuracy: 0.9029\n",
            "Epoch 29/30\n",
            "1238/1238 [==============================] - 39s 31ms/step - loss: 0.2854 - accuracy: 0.9032\n",
            "Epoch 30/30\n",
            "1238/1238 [==============================] - 40s 32ms/step - loss: 0.2748 - accuracy: 0.9061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation\n",
        "\n",
        "# importing data as text\n",
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "# read the data\n",
        "text = open(\"/content/wonderland.txt\", encoding=\"utf-8\").read()\n",
        "# remove caps, comment this code if you want uppercase characters as well\n",
        "text = text.lower()\n",
        "# remove punctuation\n",
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "\n",
        "# print some stats\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print(\"unique_chars:\", vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print(\"Number of characters:\", n_chars)\n",
        "print(\"Number of unique characters:\", n_unique_chars)\n",
        "\n",
        "# converting data into numbers manually\n",
        "\n",
        "# dictionary that converts characters to integers\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "# dictionary that converts integers to characters\n",
        "int2char = {i: c for i, c in enumerate(vocab)}\n",
        "\n",
        "# save these dictionaries for later generation\n",
        "pickle.dump(char2int, open(\"/content/char2int.pickle\", \"wb\"))\n",
        "pickle.dump(int2char, open(\"/content/int2char.pickle\", \"wb\"))\n",
        "\n",
        "# convert all text into integers\n",
        "encoded_text = np.array([char2int[c] for c in text])  # numpy array\n",
        "\n",
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text) # for effecient data-handling use tf.data API\n",
        "\n",
        "# print first 5 characters\n",
        "# for char in char_dataset.take(8):\n",
        "#     print(char.numpy(), int2char[char.numpy()])\n",
        "\n",
        "# build sequences by batching\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# print sequences\n",
        "# for sequence in sequences.take(2):\n",
        "#     print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "\n",
        "def split_sample(sample):\n",
        "    # example :\n",
        "    # sequence_length is 10\n",
        "    # sample is \"python is a great pro\" (21 length)\n",
        "    # ds will equal to ('python is ', 'a') encoded as integers\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "    for i in range(1, (len(sample)-1) // 2):\n",
        "        # first (input_, target) will be ('ython is a', ' ')\n",
        "        # second (input_, target) will be ('thon is a ', 'g')\n",
        "        # third (input_, target) will be ('hon is a g', 'r')\n",
        "        # and so on\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "# prepare inputs and targets\n",
        "dataset = sequences.flat_map(split_sample)\n",
        "\n",
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    # Example:\n",
        "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
        "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)\n",
        "\n",
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")\n",
        "\n",
        "# repeat, shuffle and batch the dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])\n",
        "# define the model path\n",
        "model_weights_path = f\"/content/wonderland-weights.h5\"\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#TRAINING THE MODEL\n",
        "\n",
        "# make results folder if does not exist yet\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "# train the model\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "# save the model\n",
        "model.save(model_weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATING TEXT\n",
        "\n",
        "import tqdm\n",
        "from keras.layers import Activation\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"/content/wonderland.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "# BASENAME = os.path.basename(FILE_PATH)\n",
        "\n",
        "seed = \"chapter xiii\"\n",
        "\n",
        "# load vocab dictionaries\n",
        "char2int = pickle.load(open(f\"/content/char2int.pickle\", \"rb\"))\n",
        "int2char = pickle.load(open(f\"/content/int2char.pickle\", \"rb\"))\n",
        "vocab_size = len(char2int)\n",
        "\n",
        "# building the model\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# load the optimal weights\n",
        "model.load_weights(f\"/content/wonderland-weights.h5\")\n",
        "\n",
        "\n",
        "s = seed\n",
        "n_chars = 400\n",
        "# generate 400 characters\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iud1aYM1AtJW",
        "outputId": "be1cd7b9-e793-42e8-8907-4c440bdc80d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating text: 100%|██████████| 400/400 [00:24<00:00, 16.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: chapter xiii\n",
            "Generated text:\n",
            "d and perhaps it was quite out of course who hap one a copy of a long wonder as she came upon a little while middle and more what youre was gone and the money if ive or one those ran and more in a caurrage the moment she had not the coulse upon the door so the poor little quistion that alice when she looked at howe will moment she had not the coulse upon the door so the poor little quistion that a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}