{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FMhS01BB_-tk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monsterglitch/Text-Generation/blob/master/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZcX8y0-oTxJ",
        "outputId": "68afabd9-fcc5-428e-8be3-efcabf3786e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Code"
      ],
      "metadata": {
        "id": "FMhS01BB_-tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "-Rzx4I1yl_Dv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q7UDbfIGrC2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a80c9d-b0ae-4bae-a859-1d26294138f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 0123456789abcdefghijklmnopqrstuvwxyz‘’“”\n",
            "Number of characters: 157893\n",
            "Number of unique characters: 42\n",
            "Input: the project gutenberg ebook of alices adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of any\n",
            "Target: o\n",
            "Input shape: (100, 42)\n",
            "Target shape: (42,)\n",
            "================================================== \n",
            "\n",
            "Input: he project gutenberg ebook of alices adventures in wonderland\n",
            "    \n",
            "this ebook is for the use of anyo\n",
            "Target: n\n",
            "Input shape: (100, 42)\n",
            "Target shape: (42,)\n",
            "================================================== \n",
            "\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 100, 256)          306176    \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 42)                10794     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 842282 (3.21 MB)\n",
            "Trainable params: 842282 (3.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# importing data as text\n",
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 50\n",
        "# read the data\n",
        "text = open(\"/content/drive/MyDrive/NN/wonderland.txt\", encoding=\"utf-8\").read()\n",
        "# remove caps, comment this code if you want uppercase characters as well\n",
        "text = text.lower()\n",
        "# remove punctuation\n",
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "spl = ['™', 'ù', '—', '•']\n",
        "for item in spl:\n",
        "  text = text.replace(item, '')\n",
        "\n",
        "# print some stats\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print(\"unique_chars:\", vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print(\"Number of characters:\", n_chars)\n",
        "print(\"Number of unique characters:\", n_unique_chars)\n",
        "\n",
        "# converting data into numbers manually\n",
        "\n",
        "# dictionary that converts characters to integers\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "# dictionary that converts integers to characters\n",
        "int2char = {i: c for i, c in enumerate(vocab)}\n",
        "\n",
        "# save these dictionaries for later generation\n",
        "pickle.dump(char2int, open(\"/content/char2int.pickle\", \"wb\"))\n",
        "pickle.dump(int2char, open(\"/content/int2char.pickle\", \"wb\"))\n",
        "\n",
        "# convert all text into integers\n",
        "encoded_text = np.array([char2int[c] for c in text])  # numpy array\n",
        "\n",
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text) # for efficient data-handling use tf.data API\n",
        "\n",
        "# print first 5 characters\n",
        "# for char in char_dataset.take(8):\n",
        "#     print(char.numpy(), int2char[char.numpy()])\n",
        "\n",
        "# build sequences by batching\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# print sequences\n",
        "# for sequence in sequences.take(2):\n",
        "#     print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "\n",
        "def split_sample(sample):\n",
        "    # example :\n",
        "    # sequence_length is 10\n",
        "    # sample is \"python is a great pro\" (21 length)\n",
        "    # ds will equal to ('python is ', 'a') encoded as integers\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "    for i in range(1, (len(sample)-1) // 2):\n",
        "        # first (input_, target) will be ('ython is a', ' ')\n",
        "        # second (input_, target) will be ('thon is a ', 'g')\n",
        "        # third (input_, target) will be ('hon is a g', 'r')\n",
        "        # and so on\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "# prepare inputs and targets\n",
        "dataset = sequences.flat_map(split_sample)\n",
        "\n",
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    # Example:\n",
        "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
        "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)\n",
        "\n",
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")\n",
        "\n",
        "# repeat, shuffle and batch the dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])\n",
        "# define the model path\n",
        "model_weights_path = f\"/content/wonderland-weights.h5\"\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#TRAINING THE MODEL\n",
        "\n",
        "# make results folder if does not exist yet\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "# save the model\n",
        "model.save(model_weights_path)"
      ],
      "metadata": {
        "id": "rEpG9tehmleu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56b017c-c5e4-4cd4-92b6-21b43fa3c72f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1232/1232 [==============================] - 44s 31ms/step - loss: 2.2937 - accuracy: 0.3433\n",
            "Epoch 2/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 1.7687 - accuracy: 0.4802\n",
            "Epoch 3/50\n",
            "1232/1232 [==============================] - 39s 31ms/step - loss: 1.5448 - accuracy: 0.5390\n",
            "Epoch 4/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 1.3808 - accuracy: 0.5846\n",
            "Epoch 5/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 1.2478 - accuracy: 0.6198\n",
            "Epoch 6/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 1.1306 - accuracy: 0.6514\n",
            "Epoch 7/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 1.0261 - accuracy: 0.6807\n",
            "Epoch 8/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 0.9286 - accuracy: 0.7092\n",
            "Epoch 9/50\n",
            "1232/1232 [==============================] - 39s 32ms/step - loss: 0.8447 - accuracy: 0.7332\n",
            "Epoch 10/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.7710 - accuracy: 0.7549\n",
            "Epoch 11/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.7016 - accuracy: 0.7759\n",
            "Epoch 12/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.6460 - accuracy: 0.7922\n",
            "Epoch 13/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.5916 - accuracy: 0.8081\n",
            "Epoch 14/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.5501 - accuracy: 0.8204\n",
            "Epoch 15/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.5128 - accuracy: 0.8324\n",
            "Epoch 16/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.4789 - accuracy: 0.8426\n",
            "Epoch 17/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.4523 - accuracy: 0.8509\n",
            "Epoch 18/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.4211 - accuracy: 0.8601\n",
            "Epoch 19/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 0.3992 - accuracy: 0.8670\n",
            "Epoch 20/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3803 - accuracy: 0.8730\n",
            "Epoch 21/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3623 - accuracy: 0.8790\n",
            "Epoch 22/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3526 - accuracy: 0.8810\n",
            "Epoch 23/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.3350 - accuracy: 0.8880\n",
            "Epoch 24/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3223 - accuracy: 0.8920\n",
            "Epoch 25/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3123 - accuracy: 0.8946\n",
            "Epoch 26/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.3069 - accuracy: 0.8968\n",
            "Epoch 27/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2904 - accuracy: 0.9019\n",
            "Epoch 28/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2843 - accuracy: 0.9035\n",
            "Epoch 29/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.2758 - accuracy: 0.9067\n",
            "Epoch 30/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 0.2691 - accuracy: 0.9088\n",
            "Epoch 31/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2629 - accuracy: 0.9113\n",
            "Epoch 32/50\n",
            "1232/1232 [==============================] - 42s 34ms/step - loss: 0.2596 - accuracy: 0.9118\n",
            "Epoch 33/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 0.2514 - accuracy: 0.9148\n",
            "Epoch 34/50\n",
            "1232/1232 [==============================] - 42s 34ms/step - loss: 0.2472 - accuracy: 0.9166\n",
            "Epoch 35/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2411 - accuracy: 0.9173\n",
            "Epoch 36/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2372 - accuracy: 0.9196\n",
            "Epoch 37/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.2290 - accuracy: 0.9221\n",
            "Epoch 38/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 0.2277 - accuracy: 0.9225\n",
            "Epoch 39/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2260 - accuracy: 0.9229\n",
            "Epoch 40/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.2176 - accuracy: 0.9263\n",
            "Epoch 41/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2162 - accuracy: 0.9262\n",
            "Epoch 42/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2136 - accuracy: 0.9272\n",
            "Epoch 43/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2059 - accuracy: 0.9300\n",
            "Epoch 44/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2016 - accuracy: 0.9317\n",
            "Epoch 45/50\n",
            "1232/1232 [==============================] - 40s 33ms/step - loss: 0.2024 - accuracy: 0.9309\n",
            "Epoch 46/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.2004 - accuracy: 0.9320\n",
            "Epoch 47/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.1900 - accuracy: 0.9349\n",
            "Epoch 48/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.1953 - accuracy: 0.9333\n",
            "Epoch 49/50\n",
            "1232/1232 [==============================] - 40s 32ms/step - loss: 0.1871 - accuracy: 0.9364\n",
            "Epoch 50/50\n",
            "1232/1232 [==============================] - 41s 33ms/step - loss: 0.1865 - accuracy: 0.9366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for sequence in sequences.take(1):\n",
        "#     # print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "#     print(sequence)\n",
        "# print()\n",
        "# for sequence in sequences.take(1):\n",
        "#     print(''.join([int2char[i] for i in sequence.numpy()]))\n",
        "for e in dataset.take(1):\n",
        "  print([np.argmax(char_vector) for char_vector in element[0].numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upnl4J3qREB7",
        "outputId": "f2aae8af-8123-464e-fd45-94c35cd77c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21, 16, 14, 31, 1, 18, 32, 31, 16, 25, 13, 16, 29, 18, 30, 1, 12, 23, 20, 14, 16, 30, 1, 12, 15, 33, 16, 25, 31, 32, 29, 16, 30, 1, 20, 25, 1, 34, 26, 25, 15, 16, 29, 23, 12, 25, 15, 1, 13, 36, 1, 23, 16, 34, 20, 30, 1, 14, 12, 29, 29, 26, 23, 23, 0, 0, 0, 0, 31, 19, 20, 30, 1, 16, 13, 26, 26, 22, 1, 20, 30, 1, 17, 26, 29, 1, 31, 19, 16, 1, 32, 30, 16, 1, 26, 17, 1, 12, 25, 36]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATING TEXT\n",
        "\n",
        "import tqdm\n",
        "from keras.layers import Activation\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"/content/wonderland.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "# BASENAME = os.path.basename(FILE_PATH)\n",
        "\n",
        "seed = \"chapter xi\"\n",
        "\n",
        "# load vocab dictionaries\n",
        "char2int = pickle.load(open(f\"/content/char2int.pickle\", \"rb\"))\n",
        "int2char = pickle.load(open(f\"/content/int2char.pickle\", \"rb\"))\n",
        "vocab_size = len(char2int)\n",
        "\n",
        "# building the model\n",
        "model1 = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# load the optimal weights\n",
        "model.load_weights(f\"/content/wonderland-weights.h5\")\n",
        "\n",
        "\n",
        "s = seed\n",
        "n_chars = 300\n",
        "# generate 400 characters\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iud1aYM1AtJW",
        "outputId": "41843b21-858e-4dfd-d0e5-ad95875473f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating text: 100%|██████████| 300/300 [00:20<00:00, 14.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: chapter xi\n",
            "Generated text:\n",
            "xs she’s looking at the shater of a long as if you find as feeling and denion compriance to see there as indeed very close\n",
            "by feeling very much crossed up and read that it here your mands” and she spread out her can follow and deristen to me and for any one we\n",
            "would be one finger and belong about it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second Code"
      ],
      "metadata": {
        "id": "UwGQU7AbAHBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/drive/MyDrive/NN/wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "raw_text = raw_text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "spl = ['™', 'ù', '—', '•']\n",
        "for item in spl:\n",
        "  raw_text = raw_text.replace(item, '')\n",
        "vocab = ''.join(sorted(set(raw_text)))\n",
        "print(\"Unique characters: \", vocab)\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "m5SQXn2N_E_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0441ff-7378-499a-af7e-6bba320a5530"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters:  \n",
            " 0123456789abcdefghijklmnopqrstuvwxyz‘’“”\n",
            "Total Characters:  157893\n",
            "Total Vocab:  42\n",
            "Total Patterns:  157793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "A1aS-hvK8Ix3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# define another LSTM model\n",
        "model1 = Sequential()\n",
        "model1.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(LSTM(256))\n",
        "# model1.add(Dropout(0.2))\n",
        "model1.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"weights.hdf5\"\n",
        "model1.load_weights(filename)\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        " x = np.reshape(pattern, (1, len(pattern), 1))\n",
        " x = x / float(n_vocab)\n",
        " prediction = model1.predict(x, verbose=0)\n",
        " index = np.argmax(prediction)\n",
        " result = int_to_char[index]\n",
        " seq_in = [int_to_char[value] for value in pattern]\n",
        " sys.stdout.write(result)\n",
        " pattern.append(index)\n",
        " pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xsMlr63omAp",
        "outputId": "9c98f2fa-f460-4637-84a1-35662c9d7018"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\"  terror “oh there goes his precious nose” as an\n",
            "unusually large saucepan flew close by it and very n \"\n",
            "early anl the way was the white rabbit with a sree \n",
            "“not i was not a rerpent” scid the caterpillar \n",
            "“i’ve hately know yhat is the same thing” said the mock turtle “seals throw then i’ve got to grow up and the other side with the tor of the court with one finger and had been of her head to she set to work the lock turtle seplied rather shat she had not a moment to she was not and the words “drink me” but the door she rat down and looked at alice\n",
            "and looked at alice\n",
            "\n",
            "“wou couldn’t halds the sreess” said the mock turtle “seals throw then i’d cettainly to shis it was a cat and the others sook them of course you know what it was shat i goow she world gate it in a lou to the cook the cook the cook the cook to she was not and the words “drink me” but the door she rat down and looked at alice\n",
            "and looked at alice\n",
            "\n",
            "“wou couldn’t halds the sreess” said the mock turtle “seals throw then i’d cettainly to shis it was a cat and the others sook them of course you know what it was shat i goow she world\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}